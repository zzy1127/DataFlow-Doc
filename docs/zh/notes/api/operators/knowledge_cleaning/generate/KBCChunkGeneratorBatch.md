---
title: KBCChunkGeneratorBatch
createTime: 2025/10/09 17:09:04
permalink: /zh/api/operators/knowledge_cleaning/generate/kbcchunkgeneratorbatch/
---

## ğŸ“˜ æ¦‚è¿°
KBCChunkGeneratorBatch æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†å‰²ç®—å­ï¼Œæ—¨åœ¨å°†é•¿æ–‡æœ¬æˆ–è¯­æ–™åº“åˆ†å‰²æˆæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å—ï¼ˆchunksï¼‰ã€‚è¯¥ç®—å­æ”¯æŒå¤šç§åˆ†å‰²ç­–ç•¥ï¼ŒåŒ…æ‹¬æŒ‰ tokenã€å¥å­ã€è¯­ä¹‰æˆ–é€’å½’æ–¹å¼è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å…è®¸ç”¨æˆ·è‡ªå®šä¹‰å—å¤§å°ã€é‡å éƒ¨åˆ†å’Œæœ€å°å—é•¿åº¦ï¼Œç‰¹åˆ«ä¸º RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åº”ç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## `__init__`å‡½æ•°
```python
def __init__(self,
             chunk_size: int = 512,
             chunk_overlap: int = 50,
             split_method: str = "token",
             min_tokens_per_chunk: int = 100,
             tokenizer_name: str = "bert-base-uncased",
             )
```
### initå‚æ•°è¯´æ˜
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
| :------------------ | :---- | :-------------------- | :------------------------------------------------ |
| **chunk_size** | int | 512 | æ¯ä¸ªæ–‡æœ¬å—çš„ç›®æ ‡å¤§å°ï¼ˆæ ¹æ®`split_method`å¯èƒ½æ˜¯tokenæ•°æˆ–å­—ç¬¦æ•°ï¼‰ã€‚ |
| **chunk_overlap** | int | 50 | ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´é‡å çš„å¤§å°ï¼Œä»¥ç¡®ä¿ä¸Šä¸‹æ–‡è¿ç»­æ€§ã€‚ |
| **split_method** | str | "token" | æ–‡æœ¬åˆ†å‰²æ–¹æ³•ã€‚å¯é€‰å€¼ä¸º "token", "sentence", "semantic", "recursive"ã€‚ |
| **min_tokens_per_chunk** | int | 100 | æ¯ä¸ªå—å…è®¸çš„æœ€å°tokenæ•°ã€‚ |
| **tokenizer_name** | str | "bert-base-uncased" | ç”¨äºtokenåˆ†å‰²å’Œtokenè®¡æ•°çš„åˆ†è¯å™¨æ¨¡å‹åç§°ã€‚ |

### åˆ†å‰²æ–¹æ³•è¯´æ˜
| åˆ†å‰²æ–¹æ³• | ä¸»è¦ç”¨é€” | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹è¯´æ˜ |
| ---------------- | ------------- | -------------------- | ----------------------------------------------------- |
| **token** | æŒ‰å›ºå®štokenæ•°é‡åˆ†å‰² | éœ€è¦ä¸¥æ ¼æ§åˆ¶è¾“å…¥é•¿åº¦çš„åœºæ™¯ | æœ€ç›´æ¥çš„åˆ†å‰²æ–¹å¼ï¼Œç¡®ä¿æ¯ä¸ªå—çš„tokenæ•°ä¸è¶…è¿‡`chunk_size`ã€‚ |
| **sentence** | æŒ‰å¥å­è¾¹ç•Œåˆ†å‰² | å¸Œæœ›ä¿æŒå¥å­å®Œæ•´æ€§çš„æ–‡æœ¬ | ä»¥å¥å­ä¸ºå•ä½è¿›è¡Œåˆ†å‰²ï¼Œé¿å…åˆ‡æ–­å®Œæ•´çš„è¯­ä¹‰å•å…ƒã€‚ |
| **semantic** | æŒ‰è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†å‰² | ä¸»é¢˜æ˜ç¡®çš„æ–‡æ¡£æˆ–æ®µè½ | é€šè¿‡è¯­ä¹‰èšç±»ï¼Œå°†è¯­ä¹‰ç›¸å…³çš„å†…å®¹åˆ’åˆ†åˆ°åŒä¸€ä¸ªå—ä¸­ã€‚ |
| **recursive** | é€’å½’åˆ†å±‚åˆ†å‰² | ç»“æ„å¤æ‚æˆ–æœªçŸ¥çš„é€šç”¨æ–‡æœ¬ | å°è¯•ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦ï¼ˆå¦‚æ®µè½ã€å¥å­ã€å•è¯ï¼‰è¿›è¡Œé€’å½’åˆ†å‰²ï¼Œæ˜¯ä¸€ç§é²æ£’çš„é€šç”¨ç­–ç•¥ã€‚ |

## `run`å‡½æ•°
```python
def run(self, storage: DataFlowStorage, input_key: str = "text_path", output_key: str = "chunk_path")
```
#### å‚æ•°
| åç§° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
| :------------- | :---------------- | :------------- | :---------------------------------------------------- |
| **storage** | DataFlowStorage | å¿…éœ€ | æ•°æ®æµå­˜å‚¨å®ä¾‹ï¼Œè´Ÿè´£è¯»å–ä¸å†™å…¥æ•°æ®ã€‚ |
| **input_key** | str | "text_path" | è¾“å…¥åˆ—åï¼Œè¯¥åˆ—åŒ…å«å¾…åˆ†å‰²çš„åŸå§‹æ–‡æœ¬æ–‡ä»¶è·¯å¾„ã€‚ |
| **output_key** | str | "chunk_path" | è¾“å‡ºåˆ—åï¼Œç”¨äºå­˜å‚¨ç”Ÿæˆçš„æ–‡æœ¬å—æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰çš„è·¯å¾„ã€‚ |

## ğŸ§  ç¤ºä¾‹ç”¨æ³•
```python

```

#### ğŸ§¾ é»˜è®¤è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰
ç®—å­æ‰§è¡Œåï¼Œä¼šåœ¨è¾“å…¥çš„ DataFrame ä¸­æ·»åŠ ä¸€ä¸ªæ–°åˆ—ï¼ˆé»˜è®¤ä¸º `chunk_path`ï¼‰ï¼Œå…¶ä¸­åŒ…å«æŒ‡å‘æ–°ç”Ÿæˆçš„ JSON æ–‡ä»¶çš„è·¯å¾„ã€‚æ¯ä¸ª JSON æ–‡ä»¶å†…éƒ¨ç»“æ„å¦‚ä¸‹ï¼š

ç¤ºä¾‹è¾“å…¥DataFrameä¸­çš„ä¸€è¡Œï¼š
```json
{
"text_path":"/path/to/your/document.txt"
}
```
ç¤ºä¾‹è¾“å‡ºDataFrameä¸­çš„ä¸€è¡Œï¼š
```json
{
"text_path":"/path/to/your/document.txt",
"chunk_path":"/path/to/your/extract/document_chunk.json"
}
```
ç¤ºä¾‹ç”Ÿæˆçš„ `document_chunk.json` æ–‡ä»¶å†…å®¹ï¼š
```json
[
    {
        "raw_chunk": "è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„å†…å®¹..."
    },
    {
        "raw_chunk": "è¿™æ˜¯ç¬¬äºŒä¸ªæ–‡æœ¬å—çš„å†…å®¹ï¼Œä¸ç¬¬ä¸€ä¸ªå—æœ‰éƒ¨åˆ†é‡å ..."
    },
    ...
]
```
