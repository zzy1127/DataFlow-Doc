import{_ as e,c as s,b as i,o as a}from"./app-BqDkFDvD.js";const l={};function n(r,t){return a(),s("div",null,t[0]||(t[0]=[i('<p><code>CondorGenerator</code> is an operator that generates Synthetic Fine-Tuning (SFT) data from scratch based on predefined knowledge tree tags. It operates in a two-stage process: first generating questions of varying difficulty levels, and then generating corresponding answers for each question.</p><h2 id="init" tabindex="-1"><a class="header-anchor" href="#init"><span><code>__init__</code></span></a></h2><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#998418;--shiki-dark:#B8A965;"> __init__</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> llm_serving</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> LLMServingABC </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;"> None</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> num_samples</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">15</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> use_task_diversity</span><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">=</span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">True</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><table><thead><tr><th style="text-align:left;">Parameter</th><th style="text-align:left;">Type</th><th style="text-align:left;">Default</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>llm_serving</strong></td><td style="text-align:left;">LLMServingABC</td><td style="text-align:left;">None</td><td style="text-align:left;">The Large Language Model serving instance, used to execute inference and generation.</td></tr><tr><td style="text-align:left;"><strong>num_samples</strong></td><td style="text-align:left;">int</td><td style="text-align:left;">15</td><td style="text-align:left;">Total number of samples to generate. It is recommended to keep this value below 5000 unless more tags are added.</td></tr><tr><td style="text-align:left;"><strong>use_task_diversity</strong></td><td style="text-align:left;">bool</td><td style="text-align:left;">True</td><td style="text-align:left;">Whether to use task scenario diversity enhancement to enrich the generated questions.</td></tr></tbody></table><h2 id="prompt-template-descriptions" tabindex="-1"><a class="header-anchor" href="#prompt-template-descriptions"><span>Prompt Template Descriptions</span></a></h2><table><thead><tr><th style="text-align:left;">Prompt Template Name</th><th style="text-align:left;">Primary Use</th><th style="text-align:left;">Applicable Scenarios</th><th style="text-align:left;">Feature Description</th></tr></thead><tbody><tr><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td><td style="text-align:left;"></td></tr></tbody></table><h2 id="run" tabindex="-1"><a class="header-anchor" href="#run"><span><code>run</code></span></a></h2><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#AB5959;--shiki-dark:#CB7676;">def</span><span style="--shiki-light:#59873A;--shiki-dark:#80A665;"> run</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">self</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> storage</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">:</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> DataFlowStorage</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><table><thead><tr><th style="text-align:left;">Parameter</th><th style="text-align:left;">Type</th><th style="text-align:left;">Default</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>storage</strong></td><td style="text-align:left;">DataFlowStorage</td><td style="text-align:left;">Required</td><td style="text-align:left;">The DataFlowStorage instance, responsible for writing the generated data.</td></tr></tbody></table><h2 id="ðŸ§ -example-usage" tabindex="-1"><a class="header-anchor" href="#ðŸ§ -example-usage"><span>ðŸ§  Example Usage</span></a></h2><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-python"><span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="ðŸ§¾-output-format" tabindex="-1"><a class="header-anchor" href="#ðŸ§¾-output-format"><span>ðŸ§¾ Output Format</span></a></h4><table><thead><tr><th style="text-align:left;">Field</th><th style="text-align:left;">Type</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;">difficulty</td><td style="text-align:left;">str</td><td style="text-align:left;">Difficulty level of the question (e.g., &quot;Easy&quot;, &quot;Medium&quot;, &quot;Hard&quot;).</td></tr><tr><td style="text-align:left;">instruction</td><td style="text-align:left;">str</td><td style="text-align:left;">The generated question text.</td></tr><tr><td style="text-align:left;">output</td><td style="text-align:left;">str</td><td style="text-align:left;">The generated answer for the corresponding instruction.</td></tr></tbody></table>',13)]))}const h=e(l,[["render",n]]),o=JSON.parse('{"path":"/en/api/operators/text_sft/generate/condorgenerator/","title":"CondorGenerator","lang":"en-US","frontmatter":{"title":"CondorGenerator","createTime":"2025/10/09 16:52:48","permalink":"/en/api/operators/text_sft/generate/condorgenerator/"},"readingTime":{"minutes":0.63,"words":190},"git":{"createdTime":1760001305000,"updatedTime":1760001305000,"contributors":[{"name":"Hao Liang","username":"","email":"hao.liang@stu.pku.edu.cn","commits":1,"avatar":"https://gravatar.com/avatar/105bae3e8661728b9f2f5440992b04f5f28459b66a049d09b52213ce1438f6bc?d=retro"}]},"filePathRelative":"en/notes/api/operators/text_sft/generate/CondorGenerator.md","headers":[]}');export{h as comp,o as data};
